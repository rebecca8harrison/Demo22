{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from time import time\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>round</th>\n",
       "      <th>acl.id.matched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl.id  X1                                               text label  \\\n",
       "1      acl1   1                                dalits are lowlives  hate   \n",
       "2     acl10   2             gay people are a burden to our society  hate   \n",
       "3    acl100   3                              Arabs are not welcome  hate   \n",
       "4   acl1000   4  I'm not saying we should actually eliminate he...  hate   \n",
       "5  acl10000   5                       bananas are for black people  hate   \n",
       "\n",
       "       type    target     level  split  round.base   annotator round  \\\n",
       "1  notgiven  notgiven  original  train           1  GNZuCtwed3     1   \n",
       "2  notgiven  notgiven  original  train           1  GNZuCtwed3     1   \n",
       "3  notgiven  notgiven  original  train           1  vDe7GN0NrL     1   \n",
       "4  notgiven  notgiven  original  train           1  oemYWm1Tjg     1   \n",
       "5  notgiven  notgiven  original   test           1  QiOKkCi7F8     1   \n",
       "\n",
       "  acl.id.matched  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "5            NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a dataset for classification that has been generated by some experts as part of the dynabench project: https://dynabench.org/tasks/hs and https://arxiv.org/abs/2012.15761\n",
    "data_gen=pd.read_csv(\"/Users/rebeccaharrison/Documents/Python/Bootcamp/Hate_speech_project/Dynamically-Generated-Hate-Speech-Dataset-main/Dynamically Generated Hate Dataset v0.2.3.csv\",index_col=0)\n",
    "data_gen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_gen = data_gen.drop(columns=['acl.id', 'X1','round.base','annotator','round','acl.id.matched'], axis=1)\n",
    "#data_gen_2_combine = data_gen.drop(columns=[ 'type','target','level','split'], axis=1)\n",
    "#data_gen_2_combine.columns = ['text', 'label']\n",
    "#data_gen_2_combine.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re labelling 1 as hate speech to combine \n",
    "#data_gen_2_combine = data_gen_2_combine.replace({'label': {'hate': 1, 'nothate': 0}})\n",
    "#data_gen_2_combine.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this a dataset that has been collated from three other sources from: https://towardsdatascience.com/how-to-create-your-own-hate-tweet-detector-704508c34cd0\n",
    "#data_comb=pd.read_csv(\"Data/combined.csv\", index_col=0)\n",
    "#data_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the compiled dataset and the one from towardsdatascience\n",
    "#combined = pd.concat([data_comb, data_gen_2_combine], ignore_index=1)\n",
    "#combined.label.value_counts()\n",
    "#combined.to_csv('/Users/rebeccaharrison/Documents/Python/Bootcamp/Data/combined_final.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>...</th>\n",
       "      <th>Eastern European</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Mixed race</th>\n",
       "      <th>Ethnic minority</th>\n",
       "      <th>Indigenous people</th>\n",
       "      <th>Non-white</th>\n",
       "      <th>Traveller</th>\n",
       "      <th>Working class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl.id  X1                                               text label  \\\n",
       "1      acl1   1                                dalits are lowlives  hate   \n",
       "2     acl10   2             gay people are a burden to our society  hate   \n",
       "3    acl100   3                              Arabs are not welcome  hate   \n",
       "4   acl1000   4  I'm not saying we should actually eliminate he...  hate   \n",
       "5  acl10000   5                       bananas are for black people  hate   \n",
       "\n",
       "       type    target     level  split  round.base   annotator  ...  \\\n",
       "1  notgiven  notgiven  original  train           1  GNZuCtwed3  ...   \n",
       "2  notgiven  notgiven  original  train           1  GNZuCtwed3  ...   \n",
       "3  notgiven  notgiven  original  train           1  vDe7GN0NrL  ...   \n",
       "4  notgiven  notgiven  original  train           1  oemYWm1Tjg  ...   \n",
       "5  notgiven  notgiven  original   test           1  QiOKkCi7F8  ...   \n",
       "\n",
       "  Eastern European Russian  Polish  Hispanic  Mixed race  Ethnic minority  \\\n",
       "1                0       0       0         0           0                0   \n",
       "2                0       0       0         0           0                0   \n",
       "3                0       0       0         0           0                0   \n",
       "4                0       0       0         0           0                0   \n",
       "5                0       0       0         0           0                0   \n",
       "\n",
       "   Indigenous people  Non-white  Traveller  Working class  \n",
       "1                  0          0          0              0  \n",
       "2                  0          0          0              0  \n",
       "3                  0          0          0              0  \n",
       "4                  0          0          0              0  \n",
       "5                  0          0          0              0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preaparing dataset for analysis with different types of hate speech\n",
    "data_gen['target'].value_counts()\n",
    "data_gen[\"target\"] = data_gen[\"target\"].fillna(\"\")\n",
    "\n",
    "# Creating variables that define target based on religion\n",
    "data_gen['Religion']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('jew'), 'Religion'] = 1 # -> Jewish people\n",
    "data_gen.loc[data_gen['target'].str.contains('mus'), 'Religion'] = 1 # -> Muslims\n",
    "data_gen.loc[data_gen['target'].str.contains('muswom'), 'Religion'] = 1 # -> Jewish people\n",
    "data_gen.loc[data_gen['target'].str.contains('other.religion'), 'Religion'] = 1 # -> other religion\n",
    "data_gen['Jewish people']=0 # -> Jewish people\n",
    "data_gen.loc[data_gen['target'].str.contains('jew'), 'Jewish people'] = 1 # -> Jewish people\n",
    "data_gen['Muslims']=0 # -> Muslims\n",
    "data_gen.loc[data_gen['target'].str.contains('mus'), 'Muslims'] = 1 # -> Muslims\n",
    "data_gen['Muslim women']=0 # -> Muslim women\n",
    "data_gen.loc[data_gen['target'].str.contains('muswom'), 'Muslim women'] = 1 # -> Muslim women\n",
    "\n",
    "#creating a class based variable (removed due to lack of data)\n",
    "#data_gen['Class']=0 # -> Working class people\n",
    "#data_gen.loc[data_gen['target'].str.contains('working'), 'Class'] = 1\n",
    "\n",
    "# Creating an ageist classification\n",
    "data_gen['Oldist']=0 # -> Elderly people\n",
    "data_gen.loc[data_gen['target'].str.contains('old'), 'Oldist'] = 1\n",
    "\n",
    "# Support of hate groups variable\n",
    "data_gen['Support']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('nazi'), 'Support'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('hitler'), 'Support'] = 1\n",
    "\n",
    "# Creating an immigration status based variable\n",
    "data_gen['Immigration status']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('immig'), 'Immigration status'] = 1 # -> Immigrants\n",
    "data_gen.loc[data_gen['target'].str.contains('asylum'), 'Immigration status'] = 1  # -> Asylum seekers\n",
    "data_gen.loc[data_gen['target'].str.contains('ref'), 'Immigration status'] = 1 # -> Refugees\n",
    "data_gen.loc[data_gen['target'].str.contains('for'), 'Immigration status'] = 1  # -> Foreigners\n",
    "\n",
    "# creating an ableness target based variable\n",
    "data_gen['Ableness / disability']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('dis'), 'Ableness / disability'] = 1  # -> People with disabilities\n",
    "\n",
    "# Creating variables that define target based on gender\n",
    "data_gen['Gender']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('muswom'), 'Gender'] = 1 # -> Muslim women\n",
    "data_gen.loc[data_gen['target'].str.contains('indigwom'), 'Gender'] = 1 # -> Indigenous Women\n",
    "data_gen.loc[data_gen['target'].str.contains('wom'), 'Gender'] = 1 # -> Women\n",
    "data_gen.loc[data_gen['target'].str.contains('blawom'), 'Gender'] = 1 # -> Black women\n",
    "data_gen.loc[data_gen['target'].str.contains('blaman'), 'Gender'] = 1 # -> Black men\n",
    "data_gen.loc[data_gen['target'].str.contains('trans'), 'Gender'] = 1 # -> Trans people\n",
    "data_gen.loc[data_gen['target'].str.contains('gendermin'), 'Gender'] = 1 # -> Gender minorities,\n",
    "data_gen.loc[data_gen['target'].str.contains('asiwom'), 'Gender'] = 1 # -> Asian women\n",
    "\n",
    "data_gen['Indigenous Women']=0 # -> Indigenous Women\n",
    "data_gen.loc[data_gen['target'].str.contains('indigwom'), 'Indigenous Women'] = 1 # -> Indigenous Women\n",
    "data_gen['Women']=0 # -> Women \n",
    "data_gen.loc[data_gen['target'].str.contains('wom'), 'Women'] = 1 # -> Women\n",
    "data_gen['Black women']=0 # -> Black women\n",
    "data_gen.loc[data_gen['target'].str.contains('blawom'), 'Black women'] = 1 # -> Black women\n",
    "data_gen['Black men']=0 # -> Black men\n",
    "data_gen.loc[data_gen['target'].str.contains('blaman'), 'Black men'] = 1 # -> Black men\n",
    "data_gen['Trans people']=0 # -> Trans people\n",
    "data_gen.loc[data_gen['target'].str.contains('trans'), 'Trans people'] = 1 # -> Trans people\n",
    "data_gen['Gender minorities']=0 # -> Gender minorities, \n",
    "data_gen.loc[data_gen['target'].str.contains('gendermin'), 'Gender minorities'] = 1 # -> Gender minorities,\n",
    "data_gen['Asian women']=0 # -> Asian women\n",
    "data_gen.loc[data_gen['target'].str.contains('asiwom'), 'Asian women'] = 1 # -> Asian women\n",
    "\n",
    "# Creating variables based on sexuality and sexual preference\n",
    "data_gen['Sexuality and Sexual preference']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('bis'), 'Sexuality and Sexual preference'] = 1 # -> Bisexual\n",
    "data_gen.loc[data_gen['target'].str.contains('gay'), 'Sexuality and Sexual preference'] = 1 # -> Gay people (both men and women)\n",
    "data_gen.loc[data_gen['target'].str.contains('gayman'), 'Sexuality and Sexual preference'] = 1 # -> Gay men\n",
    "data_gen.loc[data_gen['target'].str.contains('gaywom'), 'Sexuality and Sexual preference'] = 1 # -> Lesbians \n",
    "data_gen.loc[data_gen['target'].str.contains('lgbtq'), 'Sexuality and Sexual preference'] = 1 # -> lgbtq\n",
    "\n",
    "data_gen['Bisexual']=0 # -> Bisexual\n",
    "data_gen.loc[data_gen['target'].str.contains('bis'), 'Bisexual'] = 1 # -> Bisexual\n",
    "data_gen['Gay']=0 # -> Gay people (both men and women)\n",
    "data_gen.loc[data_gen['target'].str.contains('gay'), 'Gay'] = 1 # -> Gay people (both men and women)\n",
    "data_gen['Gay men']=0 # -> Gay men\n",
    "data_gen.loc[data_gen['target'].str.contains('gayman'), 'Gay men'] = 1 # -> Gay men\n",
    "data_gen['Gay women']=0 # -> Lesbians    \n",
    "data_gen.loc[data_gen['target'].str.contains('gaywom'), 'Gay women'] = 1 # -> Lesbians \n",
    "\n",
    "\n",
    "\n",
    "# Creating an ethnicity based variable\n",
    "data_gen['Ethnicity/Race']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('mixed'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('ethnic.minority'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('indig'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('non.white'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('trav'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('african'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('asi'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('east'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('south'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('chinese'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('pak'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('arab'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('eastern european'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('russian'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('pol'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('hispanic'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('bla'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('other.national'), 'Ethnicity/Race'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('immig'), 'Ethnicity/Race'] = 1 # -> Immigrants\n",
    "data_gen.loc[data_gen['target'].str.contains('asylum'), 'Ethnicity/Race'] = 1  # -> Asylum seekers\n",
    "data_gen.loc[data_gen['target'].str.contains('ref'), 'Ethnicity/Race'] = 1 # -> Refugees\n",
    "data_gen.loc[data_gen['target'].str.contains('for'), 'Ethnicity/Race'] = 1  # -> Foreigners\n",
    "\n",
    "# Creating a black race based variable\n",
    "data_gen['Black people']=0\n",
    "data_gen.loc[data_gen['target'].str.contains('bla'), 'Black people'] = 1\n",
    "data_gen.loc[data_gen['target'].str.contains('african'), 'Black people'] = 1\n",
    "\n",
    "# Creating other race or ethnicity based individual variables\n",
    "data_gen['Asian']=0 # -> Asians\n",
    "data_gen.loc[data_gen['target'].str.contains('asi'), 'Asian'] = 1\n",
    "data_gen['East asian']=0 # -> East Asians\n",
    "data_gen.loc[data_gen['target'].str.contains('east'), 'East asian'] = 1\n",
    "data_gen['South Asians']=0 # -> South Asians (e.g. Indians)\n",
    "data_gen.loc[data_gen['target'].str.contains('south'), 'South Asian'] = 1\n",
    "data_gen['Chinese']=0 # -> Chinese people\n",
    "data_gen.loc[data_gen['target'].str.contains('chinese'), 'Chinese'] = 1\n",
    "data_gen['Pakistani']=0 # -> Pakistanis\n",
    "data_gen.loc[data_gen['target'].str.contains('pak'), 'Pakistani'] = 1\n",
    "data_gen['Arab']=0 # -> Arabs, including people from the Middle East\n",
    "data_gen.loc[data_gen['target'].str.contains('arab'), 'Arab'] = 1\n",
    "data_gen['Eastern European']=0 # -> Eastern Europeans\n",
    "data_gen.loc[data_gen['target'].str.contains('eastern european'), 'Eastern European'] = 1\n",
    "data_gen['Russian']=0 # -> Russian people\n",
    "data_gen.loc[data_gen['target'].str.contains('russian'), 'Russian'] = 1\n",
    "data_gen['Polish']=0 # -> Polish people\n",
    "data_gen.loc[data_gen['target'].str.contains('pol'), 'Polish'] = 1\n",
    "data_gen['Hispanic']=0 # -> Hispanic people, including latinx and Mexicans\n",
    "data_gen.loc[data_gen['target'].str.contains('hispanic'), 'Hispanic'] = 1\n",
    "\n",
    "data_gen['Mixed race']=0 #-> Mixed race background\n",
    "data_gen.loc[data_gen['target'].str.contains('mixed'), 'Mixed race'] = 1\n",
    "data_gen['Ethnic minority']=0 # -> Ethnic Minorities\n",
    "data_gen.loc[data_gen['target'].str.contains('ethnic minority'), 'Ethnic minority'] = 1\n",
    "data_gen['Indigenous people']=0 # -> Indigenous people\n",
    "data_gen.loc[data_gen['target'].str.contains('indig'), 'Indigenous people'] = 1\n",
    "\n",
    "data_gen['Non-white']=0 # -> Non-whites (attacked as 'non-whites', rather than specific non-white groups which are generally addressed separately)\n",
    "data_gen.loc[data_gen['target'].str.contains('non-white'), 'Non-white'] = 1\n",
    "data_gen['Traveller']=0 # -> Travellers (including Roma, gypsies)\n",
    "data_gen.loc[data_gen['target'].str.contains('trav'), 'Traveller'] = 1\n",
    "\n",
    "data_gen['Working class']=0 # -> Travellers (including Roma, gypsies)\n",
    "data_gen.loc[data_gen['target'].str.contains('wc'), 'Working class'] = 1\n",
    "\n",
    "\n",
    "data_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>...</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Mixed race</th>\n",
       "      <th>Ethnic minority</th>\n",
       "      <th>Indigenous people</th>\n",
       "      <th>Non-white</th>\n",
       "      <th>Traveller</th>\n",
       "      <th>Working class</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dalits lowlives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gay people burden society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Arabs welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm saying actually eliminate heebs.. I wish n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bananas black people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl.id  X1                                               text label  \\\n",
       "1      acl1   1                                dalits are lowlives  hate   \n",
       "2     acl10   2             gay people are a burden to our society  hate   \n",
       "3    acl100   3                              Arabs are not welcome  hate   \n",
       "4   acl1000   4  I'm not saying we should actually eliminate he...  hate   \n",
       "5  acl10000   5                       bananas are for black people  hate   \n",
       "\n",
       "       type    target     level  split  round.base   annotator  ... Russian  \\\n",
       "1  notgiven  notgiven  original  train           1  GNZuCtwed3  ...       0   \n",
       "2  notgiven  notgiven  original  train           1  GNZuCtwed3  ...       0   \n",
       "3  notgiven  notgiven  original  train           1  vDe7GN0NrL  ...       0   \n",
       "4  notgiven  notgiven  original  train           1  oemYWm1Tjg  ...       0   \n",
       "5  notgiven  notgiven  original   test           1  QiOKkCi7F8  ...       0   \n",
       "\n",
       "  Polish  Hispanic  Mixed race  Ethnic minority  Indigenous people  Non-white  \\\n",
       "1      0         0           0                0                  0          0   \n",
       "2      0         0           0                0                  0          0   \n",
       "3      0         0           0                0                  0          0   \n",
       "4      0         0           0                0                  0          0   \n",
       "5      0         0           0                0                  0          0   \n",
       "\n",
       "   Traveller  Working class                                        text_nostop  \n",
       "1          0              0                                    dalits lowlives  \n",
       "2          0              0                          gay people burden society  \n",
       "3          0              0                                      Arabs welcome  \n",
       "4          0              0  I'm saying actually eliminate heebs.. I wish n...  \n",
       "5          0              0                               bananas black people  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords from text\n",
    "\n",
    "# Get a list of stopwords in English\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Remove stopwords from the 'text' column in the dataframe\n",
    "data_gen['text_nostop'] = data_gen['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "data_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data_gen.replace({'label': {'hate': 1, 'nothate': 0}})\n",
    "data_gen = data_gen.rename(columns={'label':'Hate speech'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    0\n",
       "5    2\n",
       "Name: hurtlex, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the hate words\n",
    "search_words = pd.read_csv('/Users/rebeccaharrison/Documents/Python/Bootcamp/Hate_speech_project/Dynamically-Generated-Hate-Speech-Dataset-main/hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv', sep='\\t')\n",
    "\n",
    "categories=search_words['category'].unique()\n",
    "categories\n",
    "search_words.head()\n",
    "\n",
    "# count the number of times words from the hurtlex appear\n",
    "\n",
    "# by category\n",
    "for cat in categories:\n",
    "    word_list=\"(\" + \"|\".join(list(search_words['lemma'].loc[search_words['category']==cat])) + \")\"\n",
    "    data_gen[cat] = data_gen['text_nostop'].str.count(word_list)\n",
    "\n",
    "# overall\n",
    "words=\"(\" + \"|\".join(list(search_words['lemma'])) + \")\"\n",
    "data_gen['hurtlex'] = data_gen['text_nostop'].str.count(words)\n",
    "data_gen['hurtlex'].head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>Hate speech</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>...</th>\n",
       "      <th>asm</th>\n",
       "      <th>pr</th>\n",
       "      <th>dmc</th>\n",
       "      <th>asf</th>\n",
       "      <th>ddp</th>\n",
       "      <th>ps</th>\n",
       "      <th>pa</th>\n",
       "      <th>rci</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hurtlex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117100</th>\n",
       "      <td>acl9974</td>\n",
       "      <td>41228</td>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118100</th>\n",
       "      <td>acl11574</td>\n",
       "      <td>1752</td>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>1</td>\n",
       "      <td>derogation</td>\n",
       "      <td>wom</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119100</th>\n",
       "      <td>acl13446</td>\n",
       "      <td>3832</td>\n",
       "      <td>fuck covid</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>acl13457</td>\n",
       "      <td>3844</td>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>eLGzdD8Tvb</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121100</th>\n",
       "      <td>acl32095</td>\n",
       "      <td>24553</td>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>1</td>\n",
       "      <td>threatening</td>\n",
       "      <td>mus</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>IBsVsBliwX</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl.id     X1                                               text  \\\n",
       "1           acl1      1                                dalits are lowlives   \n",
       "2          acl10      2             gay people are a burden to our society   \n",
       "3         acl100      3                              Arabs are not welcome   \n",
       "4        acl1000      4  I'm not saying we should actually eliminate he...   \n",
       "5       acl10000      5                       bananas are for black people   \n",
       "...          ...    ...                                                ...   \n",
       "117100   acl9974  41228                     Indian culture is so backwards   \n",
       "118100  acl11574   1752  Women don't want equality, they just want to b...   \n",
       "119100  acl13446   3832                                         fuck covid   \n",
       "1205    acl13457   3844                           This computer is garbage   \n",
       "121100  acl32095  24553                 The only good Muslim is a dead one   \n",
       "\n",
       "        Hate speech         type    target     level  split  round.base  \\\n",
       "1                 1     notgiven  notgiven  original  train           1   \n",
       "2                 1     notgiven  notgiven  original  train           1   \n",
       "3                 1     notgiven  notgiven  original  train           1   \n",
       "4                 1     notgiven  notgiven  original  train           1   \n",
       "5                 1     notgiven  notgiven  original   test           1   \n",
       "...             ...          ...       ...       ...    ...         ...   \n",
       "117100            1     notgiven  notgiven  original   test           1   \n",
       "118100            1   derogation       wom  original  train           2   \n",
       "119100            0         none      none  original  train           2   \n",
       "1205              0         none      none  original  train           2   \n",
       "121100            1  threatening       mus  original  train           4   \n",
       "\n",
       "         annotator  ... asm pr  dmc  asf  ddp  ps  pa  rci  ddf  hurtlex  \n",
       "1       GNZuCtwed3  ...   0  0    1    0    0   0   0    0    0        1  \n",
       "2       GNZuCtwed3  ...   0  0    0    0    0   0   0    0    0        2  \n",
       "3       vDe7GN0NrL  ...   0  0    0    0    0   0   0    0    0        0  \n",
       "4       oemYWm1Tjg  ...   0  0    0    0    0   0   0    0    0        0  \n",
       "5       QiOKkCi7F8  ...   0  0    1    0    0   1   0    0    0        2  \n",
       "...            ...  ...  .. ..  ...  ...  ...  ..  ..  ...  ...      ...  \n",
       "117100  GNZuCtwed3  ...   0  0    0    0    0   0   0    0    0        0  \n",
       "118100  CAgNlUizNm  ...   0  0    0    0    0   0   0    0    0        0  \n",
       "119100  CAgNlUizNm  ...   1  0    0    0    0   0   0    0    0        1  \n",
       "1205    eLGzdD8Tvb  ...   0  0    0    0    0   0   0    0    0        1  \n",
       "121100  IBsVsBliwX  ...   0  0    0    0    0   0   0    0    0        1  \n",
       "\n",
       "[41144 rows x 71 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>Hate speech</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>...</th>\n",
       "      <th>pr</th>\n",
       "      <th>dmc</th>\n",
       "      <th>asf</th>\n",
       "      <th>ddp</th>\n",
       "      <th>ps</th>\n",
       "      <th>pa</th>\n",
       "      <th>rci</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>capital_letters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl.id  X1                                               text  \\\n",
       "1      acl1   1                                dalits are lowlives   \n",
       "2     acl10   2             gay people are a burden to our society   \n",
       "3    acl100   3                              Arabs are not welcome   \n",
       "4   acl1000   4  I'm not saying we should actually eliminate he...   \n",
       "5  acl10000   5                       bananas are for black people   \n",
       "\n",
       "   Hate speech      type    target     level  split  round.base   annotator  \\\n",
       "1            1  notgiven  notgiven  original  train           1  GNZuCtwed3   \n",
       "2            1  notgiven  notgiven  original  train           1  GNZuCtwed3   \n",
       "3            1  notgiven  notgiven  original  train           1  vDe7GN0NrL   \n",
       "4            1  notgiven  notgiven  original  train           1  oemYWm1Tjg   \n",
       "5            1  notgiven  notgiven  original   test           1  QiOKkCi7F8   \n",
       "\n",
       "   ... pr dmc  asf  ddp  ps  pa  rci  ddf  hurtlex  capital_letters  \n",
       "1  ...  0   1    0    0   0   0    0    0        1                0  \n",
       "2  ...  0   0    0    0   0   0    0    0        2                0  \n",
       "3  ...  0   0    0    0   0   0    0    0        0                1  \n",
       "4  ...  0   0    0    0   0   0    0    0        0                2  \n",
       "5  ...  0   1    0    0   1   0    0    0        2                0  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count capital letters\n",
    "data_gen['capital_letters'] = data_gen['text_nostop'].str.findall(r'[A-Z]').str.len()\n",
    "data_gen['capital_letters']\n",
    "data_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>Hate speech</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>...</th>\n",
       "      <th>ps</th>\n",
       "      <th>pa</th>\n",
       "      <th>rci</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl1</td>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acl10</td>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acl100</td>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>vDe7GN0NrL</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acl1000</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>oemYWm1Tjg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acl10000</td>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl.id  X1                                               text  \\\n",
       "1      acl1   1                                dalits are lowlives   \n",
       "2     acl10   2             gay people are a burden to our society   \n",
       "3    acl100   3                              Arabs are not welcome   \n",
       "4   acl1000   4  I'm not saying we should actually eliminate he...   \n",
       "5  acl10000   5                       bananas are for black people   \n",
       "\n",
       "   Hate speech      type    target     level  split  round.base   annotator  \\\n",
       "1            1  notgiven  notgiven  original  train           1  GNZuCtwed3   \n",
       "2            1  notgiven  notgiven  original  train           1  GNZuCtwed3   \n",
       "3            1  notgiven  notgiven  original  train           1  vDe7GN0NrL   \n",
       "4            1  notgiven  notgiven  original  train           1  oemYWm1Tjg   \n",
       "5            1  notgiven  notgiven  original   test           1  QiOKkCi7F8   \n",
       "\n",
       "   ... ps pa  rci  ddf  hurtlex  capital_letters    neg    neu    pos  \\\n",
       "1  ...  0  0    0    0        1                0  0.756  0.244  0.000   \n",
       "2  ...  0  0    0    0        2                0  0.492  0.508  0.000   \n",
       "3  ...  0  0    0    0        0                1  0.000  0.250  0.750   \n",
       "4  ...  0  0    0    0        0                2  0.000  0.748  0.252   \n",
       "5  ...  1  0    0    0        2                0  0.000  1.000  0.000   \n",
       "\n",
       "   compound  \n",
       "1   -0.4767  \n",
       "2   -0.4404  \n",
       "3    0.4588  \n",
       "4    0.4019  \n",
       "5    0.0000  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run sentiment analyser from NLTK\n",
    "data_gen['polarity'] = data_gen['text_nostop'].apply(lambda x: sia.polarity_scores(str(x)))\n",
    "# Change data structure so its not nested\n",
    "data_gen = pd.concat(\n",
    "    [data_gen.drop([\n",
    "       'polarity'], axis=1), \n",
    "     data_gen['polarity'].apply(pd.Series)], axis=1)\n",
    "data_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=data_gen.sort_values('X1')\n",
    "data_gen=data_gen.reset_index()\n",
    "data_gen=data_gen.drop(['index','acl.id','X1','round.base','annotator','round','acl.id.matched'],axis=1)\n",
    "\n",
    "data_gen.to_csv('prepared_hate_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Hate speech</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Jewish people</th>\n",
       "      <th>Muslims</th>\n",
       "      <th>Muslim women</th>\n",
       "      <th>...</th>\n",
       "      <th>ps</th>\n",
       "      <th>pa</th>\n",
       "      <th>rci</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>I hate drags lol</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>My washing machine is bitch</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>Fulham is just a bunch of losers</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>K I L L the T R A N S</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>gay people are the real threat to society</td>\n",
       "      <td>1</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>original</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Hate speech  \\\n",
       "0                                    dalits are lowlives            1   \n",
       "1                 gay people are a burden to our society            1   \n",
       "2                                  Arabs are not welcome            1   \n",
       "3      I'm not saying we should actually eliminate he...            1   \n",
       "4                           bananas are for black people            1   \n",
       "...                                                  ...          ...   \n",
       "41139                                   I hate drags lol            0   \n",
       "41140                        My washing machine is bitch            0   \n",
       "41141                   Fulham is just a bunch of losers            0   \n",
       "41142                              K I L L the T R A N S            1   \n",
       "41143          gay people are the real threat to society            1   \n",
       "\n",
       "           type    target     level  split  Religion  Jewish people  Muslims  \\\n",
       "0      notgiven  notgiven  original  train         0              0        0   \n",
       "1      notgiven  notgiven  original  train         0              0        0   \n",
       "2      notgiven  notgiven  original  train         0              0        0   \n",
       "3      notgiven  notgiven  original  train         0              0        0   \n",
       "4      notgiven  notgiven  original   test         0              0        0   \n",
       "...         ...       ...       ...    ...       ...            ...      ...   \n",
       "41139      none      none  original   test         0              0        0   \n",
       "41140      none      none  original   test         0              0        0   \n",
       "41141      none      none  original   test         0              0        0   \n",
       "41142  notgiven  notgiven  original   test         0              0        0   \n",
       "41143  notgiven  notgiven  original   test         0              0        0   \n",
       "\n",
       "       Muslim women  ...  ps  pa  rci  ddf  hurtlex  capital_letters    neg  \\\n",
       "0                 0  ...   0   0    0    0        1                0  0.756   \n",
       "1                 0  ...   0   0    0    0        2                0  0.492   \n",
       "2                 0  ...   0   0    0    0        0                1  0.000   \n",
       "3                 0  ...   0   0    0    0        0                2  0.000   \n",
       "4                 0  ...   1   0    0    0        2                0  0.000   \n",
       "...             ...  ...  ..  ..  ...  ...      ...              ...    ...   \n",
       "41139             0  ...   0   0    0    0        1                1  0.659   \n",
       "41140             0  ...   0   0    0    0        2                1  0.559   \n",
       "41141             0  ...   0   0    0    0        1                1  0.630   \n",
       "41142             0  ...   0   0    0    0        0                9  0.000   \n",
       "41143             0  ...   0   0    0    0        2                0  0.459   \n",
       "\n",
       "         neu    pos  compound  \n",
       "0      0.244  0.000   -0.4767  \n",
       "1      0.508  0.000   -0.4404  \n",
       "2      0.250  0.750    0.4588  \n",
       "3      0.748  0.252    0.4019  \n",
       "4      1.000  0.000    0.0000  \n",
       "...      ...    ...       ...  \n",
       "41139  0.000  0.341   -0.3818  \n",
       "41140  0.441  0.000   -0.5859  \n",
       "41141  0.370  0.000   -0.5267  \n",
       "41142  0.000  0.000    0.0000  \n",
       "41143  0.541  0.000   -0.5267  \n",
       "\n",
       "[41144 rows x 70 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/j_v3f_rs4hlbh_rnzsc6j2rm0000gn/T/ipykernel_1983/1548800313.py:2: DtypeWarning: Columns (4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  twitter=pd.read_csv(\"/Users/rebeccaharrison/Documents/Python/Bootcamp/Hate_speech_project/twitter_clean.csv\")\n"
     ]
    }
   ],
   "source": [
    "# importing and preparing the twitter and reddit data sets for analysis\n",
    "twitter=pd.read_csv(\"/Users/rebeccaharrison/Documents/Python/Bootcamp/Hate_speech_project/twitter_clean.csv\")\n",
    "reddit=pd.read_csv(\"/Users/rebeccaharrison/Documents/Python/Bootcamp/Hate_speech_project/reddit_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670707019.0</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706978.0</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706906.0</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706846.0</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706802.0</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39288</th>\n",
       "      <td>39288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670214814.0</td>\n",
       "      <td>Japan vs Korea would amazing</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39289</th>\n",
       "      <td>39289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670235006.0</td>\n",
       "      <td>This Belgium Germany got knocked</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39290</th>\n",
       "      <td>39290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670214868.0</td>\n",
       "      <td>Yes! It would extremely fun I’d happy whoever ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39291</th>\n",
       "      <td>39291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670198597.0</td>\n",
       "      <td>At international level, FIFA four official lan...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39292</th>\n",
       "      <td>39292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670202669.0</td>\n",
       "      <td>yes. weird switzerland vs serbia xhaka talking...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39293 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 subreddit       created  \\\n",
       "0               0  worldcup  1670707019.0   \n",
       "1               1  worldcup  1670706978.0   \n",
       "2               2  worldcup  1670706906.0   \n",
       "3               3  worldcup  1670706846.0   \n",
       "4               4  worldcup  1670706802.0   \n",
       "...           ...       ...           ...   \n",
       "39288       39288       NaN  1670214814.0   \n",
       "39289       39289       NaN  1670235006.0   \n",
       "39290       39290       NaN  1670214868.0   \n",
       "39291       39291       NaN  1670198597.0   \n",
       "39292       39292       NaN  1670202669.0   \n",
       "\n",
       "                                                    text language  \n",
       "0      Exciting! France advances play Croatia, realit...       en  \n",
       "1      Bellingham good nothing special. His nationali...       en  \n",
       "2      Congratulations Budweiser Man Match, France’s ...       en  \n",
       "3      Does google already know final game? Or typo? ...       en  \n",
       "4        We getting rematch 2018 something different?nan       en  \n",
       "...                                                  ...      ...  \n",
       "39288                       Japan vs Korea would amazing       en  \n",
       "39289                   This Belgium Germany got knocked       en  \n",
       "39290  Yes! It would extremely fun I’d happy whoever ...       en  \n",
       "39291  At international level, FIFA four official lan...       en  \n",
       "39292  yes. weird switzerland vs serbia xhaka talking...       en  \n",
       "\n",
       "[39293 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>user</th>\n",
       "      <th>verified</th>\n",
       "      <th>rule</th>\n",
       "      <th>language</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.601650e+18</td>\n",
       "      <td>2022-12-10T18:29:58.000Z</td>\n",
       "      <td>WE LOVE LIAM PAYNE 💗\\n #EnglandVsFrance #Engla...</td>\n",
       "      <td>False</td>\n",
       "      <td>Judit</td>\n",
       "      <td>False</td>\n",
       "      <td>England France</td>\n",
       "      <td>en</td>\n",
       "      <td>WE LOVE LIAM PAYNE 💗 #EnglandVsFrance #England...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.601650e+18</td>\n",
       "      <td>2022-12-10T18:29:57.000Z</td>\n",
       "      <td>What has Islam got to do with it?</td>\n",
       "      <td>False</td>\n",
       "      <td>Antonio Plescia</td>\n",
       "      <td>False</td>\n",
       "      <td>Worldcup</td>\n",
       "      <td>en</td>\n",
       "      <td>What Islam got it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1.601650e+18</td>\n",
       "      <td>2022-12-10T18:29:57.000Z</td>\n",
       "      <td>2002. That was also the Cup where Nigeria bo...</td>\n",
       "      <td>False</td>\n",
       "      <td>Shiv Ramdas Traing To Rite Buk</td>\n",
       "      <td>True</td>\n",
       "      <td>England France</td>\n",
       "      <td>en</td>\n",
       "      <td>2002. That also Cup Nigeria bounced Argentina....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>1.601650e+18</td>\n",
       "      <td>2022-12-10T18:29:58.000Z</td>\n",
       "      <td>Grant Wahl was a healthy individual just like...</td>\n",
       "      <td>False</td>\n",
       "      <td>Porfirio Diaz</td>\n",
       "      <td>False</td>\n",
       "      <td>Worldcup</td>\n",
       "      <td>en</td>\n",
       "      <td>Grant Wahl healthy individual like many thousa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1.601650e+18</td>\n",
       "      <td>2022-12-10T18:29:58.000Z</td>\n",
       "      <td>#ssfootball France 🇫🇷 2 England 🇬🇧 1</td>\n",
       "      <td>False</td>\n",
       "      <td>Vinny Munda</td>\n",
       "      <td>False</td>\n",
       "      <td>England France</td>\n",
       "      <td>en</td>\n",
       "      <td>#ssfootball France 🇫🇷 2 England 🇬🇧 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            id                   created  \\\n",
       "0           5  1.601650e+18  2022-12-10T18:29:58.000Z   \n",
       "1           6  1.601650e+18  2022-12-10T18:29:57.000Z   \n",
       "2          11  1.601650e+18  2022-12-10T18:29:57.000Z   \n",
       "3          13  1.601650e+18  2022-12-10T18:29:58.000Z   \n",
       "4          18  1.601650e+18  2022-12-10T18:29:58.000Z   \n",
       "\n",
       "                                                text possibly_sensitive  \\\n",
       "0  WE LOVE LIAM PAYNE 💗\\n #EnglandVsFrance #Engla...              False   \n",
       "1                  What has Islam got to do with it?              False   \n",
       "2    2002. That was also the Cup where Nigeria bo...              False   \n",
       "3   Grant Wahl was a healthy individual just like...              False   \n",
       "4               #ssfootball France 🇫🇷 2 England 🇬🇧 1              False   \n",
       "\n",
       "                             user verified            rule language  \\\n",
       "0                           Judit    False  England France       en   \n",
       "1                 Antonio Plescia    False        Worldcup       en   \n",
       "2  Shiv Ramdas Traing To Rite Buk     True  England France       en   \n",
       "3                   Porfirio Diaz    False        Worldcup       en   \n",
       "4                     Vinny Munda    False  England France       en   \n",
       "\n",
       "                                         text_nostop  \n",
       "0  WE LOVE LIAM PAYNE 💗 #EnglandVsFrance #England...  \n",
       "1                                 What Islam got it?  \n",
       "2  2002. That also Cup Nigeria bounced Argentina....  \n",
       "3  Grant Wahl healthy individual like many thousa...  \n",
       "4               #ssfootball France 🇫🇷 2 England 🇬🇧 1  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords from text\n",
    "\n",
    "# Remove stopwords from the 'text' column in the dataframe\n",
    "twitter['text_nostop'] = twitter['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "twitter.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/j_v3f_rs4hlbh_rnzsc6j2rm0000gn/T/ipykernel_1983/148079104.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit['text_nostop'] = reddit['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670707019.0</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>en</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706978.0</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>en</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706906.0</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706846.0</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706802.0</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>en</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 subreddit       created  \\\n",
       "0           0  worldcup  1670707019.0   \n",
       "1           1  worldcup  1670706978.0   \n",
       "2           2  worldcup  1670706906.0   \n",
       "3           3  worldcup  1670706846.0   \n",
       "4           4  worldcup  1670706802.0   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Exciting! France advances play Croatia, realit...       en   \n",
       "1  Bellingham good nothing special. His nationali...       en   \n",
       "2  Congratulations Budweiser Man Match, France’s ...       en   \n",
       "3  Does google already know final game? Or typo? ...       en   \n",
       "4    We getting rematch 2018 something different?nan       en   \n",
       "\n",
       "                                         text_nostop  \n",
       "0  Exciting! France advances play Croatia, realit...  \n",
       "1  Bellingham good nothing special. His nationali...  \n",
       "2  Congratulations Budweiser Man Match, France’s ...  \n",
       "3  Does google already know final game? Or typo? ...  \n",
       "4    We getting rematch 2018 something different?nan  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords from the 'text' column in the dataframe\n",
    "reddit=reddit.dropna(subset=['text'])\n",
    "reddit['text_nostop'] = reddit['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "reddit=reddit.dropna(subset=['text_nostop'])\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/rebeccaharrison/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    2\n",
       "2    1\n",
       "3    3\n",
       "4    2\n",
       "Name: hurtlex, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "categories=search_words['category'].unique()\n",
    "categories\n",
    "search_words.head()\n",
    "\n",
    "# count the number of times words from the hurtlex appear\n",
    "\n",
    "# by category\n",
    "for cat in categories:\n",
    "    word_list=\"(\" + \"|\".join(list(search_words['lemma'].loc[search_words['category']==cat])) + \")\"\n",
    "    twitter[cat] = twitter['text'].str.count(word_list)\n",
    "\n",
    "# overall\n",
    "words=\"(\" + \"|\".join(list(search_words['lemma'])) + \")\"\n",
    "twitter['hurtlex'] = twitter['text'].str.count(words)\n",
    "twitter['hurtlex'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>capital_letters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670707019.0</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>en</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706978.0</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>en</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706906.0</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706846.0</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706802.0</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>en</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 subreddit       created  \\\n",
       "0           0  worldcup  1670707019.0   \n",
       "1           1  worldcup  1670706978.0   \n",
       "2           2  worldcup  1670706906.0   \n",
       "3           3  worldcup  1670706846.0   \n",
       "4           4  worldcup  1670706802.0   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Exciting! France advances play Croatia, realit...       en   \n",
       "1  Bellingham good nothing special. His nationali...       en   \n",
       "2  Congratulations Budweiser Man Match, France’s ...       en   \n",
       "3  Does google already know final game? Or typo? ...       en   \n",
       "4    We getting rematch 2018 something different?nan       en   \n",
       "\n",
       "                                         text_nostop  capital_letters  \n",
       "0  Exciting! France advances play Croatia, realit...                5  \n",
       "1  Bellingham good nothing special. His nationali...                4  \n",
       "2  Congratulations Budweiser Man Match, France’s ...                7  \n",
       "3  Does google already know final game? Or typo? ...                2  \n",
       "4    We getting rematch 2018 something different?nan                1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count capital letters\n",
    "twitter['capital_letters'] = twitter['text_nostop'].str.findall(r'[A-Z]').str.len()\n",
    "twitter['capital_letters']\n",
    "twitter.head()\n",
    "\n",
    "\n",
    "reddit['capital_letters'] = reddit['text_nostop'].str.findall(r'[A-Z]').str.len()\n",
    "reddit['capital_letters']\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    1\n",
       "Name: hurtlex, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=search_words['category'].unique()\n",
    "categories\n",
    "search_words.head()\n",
    "\n",
    "# count the number of times words from the hurtlex appear\n",
    "\n",
    "# by category\n",
    "for cat in categories:\n",
    "    word_list= words=\"(\" + \"|\".join(list(search_words['lemma'].loc[search_words['category']==cat])) + \")\"\n",
    "    reddit[cat] = reddit['text'].str.count(word_list)\n",
    "\n",
    "# overall\n",
    "words=\"(\" + \"|\".join(list(search_words['lemma'])) + \")\"\n",
    "reddit['hurtlex'] = reddit['text'].str.count(words)\n",
    "reddit['hurtlex'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>qas</th>\n",
       "      <th>cds</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>ddp</th>\n",
       "      <th>ps</th>\n",
       "      <th>pa</th>\n",
       "      <th>rci</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670707019.0</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>en</td>\n",
       "      <td>Exciting! France advances play Croatia, realit...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.7393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706978.0</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>en</td>\n",
       "      <td>Bellingham good nothing special. His nationali...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.3699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706906.0</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Congratulations Budweiser Man Match, France’s ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.6360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706846.0</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Does google already know final game? Or typo? ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worldcup</td>\n",
       "      <td>1670706802.0</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>en</td>\n",
       "      <td>We getting rematch 2018 something different?nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 subreddit       created  \\\n",
       "0           0  worldcup  1670707019.0   \n",
       "1           1  worldcup  1670706978.0   \n",
       "2           2  worldcup  1670706906.0   \n",
       "3           3  worldcup  1670706846.0   \n",
       "4           4  worldcup  1670706802.0   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Exciting! France advances play Croatia, realit...       en   \n",
       "1  Bellingham good nothing special. His nationali...       en   \n",
       "2  Congratulations Budweiser Man Match, France’s ...       en   \n",
       "3  Does google already know final game? Or typo? ...       en   \n",
       "4    We getting rematch 2018 something different?nan       en   \n",
       "\n",
       "                                         text_nostop  capital_letters  qas  \\\n",
       "0  Exciting! France advances play Croatia, realit...                5    0   \n",
       "1  Bellingham good nothing special. His nationali...                4    0   \n",
       "2  Congratulations Budweiser Man Match, France’s ...                7    0   \n",
       "3  Does google already know final game? Or typo? ...                2    0   \n",
       "4    We getting rematch 2018 something different?nan                1    0   \n",
       "\n",
       "   cds  is  ...  ddp  ps  pa  rci  ddf  hurtlex    neg    neu    pos  compound  \n",
       "0    1   0  ...    0   0   0    0    0        2  0.000  0.590  0.410    0.7393  \n",
       "1    1   0  ...    0   0   0    0    0        1  0.132  0.586  0.281    0.3699  \n",
       "2    2   0  ...    1   0   0    0    0        2  0.000  0.589  0.411    0.6360  \n",
       "3    1   0  ...    0   0   0    0    1        3  0.000  1.000  0.000    0.0000  \n",
       "4    0   0  ...    0   0   0    0    0        1  0.000  1.000  0.000    0.0000  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# run sentiment analyser from NLTK\n",
    "twitter['polarity'] = twitter['text_nostop'].apply(lambda x: sia.polarity_scores(str(x)))\n",
    "# Change data structure so its not nested\n",
    "twitter = pd.concat(\n",
    "    [twitter.drop([\n",
    "       'polarity'], axis=1), \n",
    "     twitter['polarity'].apply(pd.Series)], axis=1)\n",
    "twitter.head()\n",
    "reddit['polarity'] = reddit['text_nostop'].apply(lambda x: sia.polarity_scores(str(x)))\n",
    "# Change data structure\n",
    "reddit = pd.concat(\n",
    "    [reddit.drop([ 'polarity'], axis=1), \n",
    "     reddit['polarity'].apply(pd.Series)], axis=1)\n",
    "reddit\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter=twitter.dropna(subset=['text_nostop'])\n",
    "reddit=reddit.dropna(subset=['text_nostop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter=twitter.reset_index()\n",
    "reddit=reddit.reset_index()\n",
    "twitter=twitter.drop(['index'],axis=1)\n",
    "reddit=reddit.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv('reddit_prepared.csv')\n",
    "twitter.to_csv('twitter_prepared.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
